# Self-instruct
A repository to perform self-instruct with a model on HF Hub

# What is this about?
This repository is dedicated to [Self-instruct](https://arxiv.org/pdf/2212.10560.pdf). It is an iterative approach which allows to generate a dataset of instructions by boostrapping on a model's prediction. For it to work well, the model used has to be powerful. The original work actually focuses on OpenAI's `text-davinci-003` engine which is one of their most powerful model. Our aim is to give a chance to modest, decoder-based models to be used for a data generation purpose.

# News

* **May 24, 2023:** We've build a space which allow to visualize the data generated by self-instruct when the model use is [StarCoderðŸ’«](https://arxiv.org/pdf/2305.06161.pdf), the recent SOTA open-source code LLM by Hugging Face ðŸ¤—.

# Disclaimer

- Our approach requires the availability of a significant amount of computational resources.
- We will focus on the dataset generation pipeline and the curation rather than the fine-tuning.
- Keep in mind that the quality of the dataset obtained by this method is strongly dependent on the quality of the model that is used. 

# Table of Contents
1. [Overview of the method](#overview)
2. [Related work](#related-work)
3. [Our approach](#our-approach)
    - [The prompting format](#the-prompting-format)
    - [The trigger words](#the-trigger-words)
    - [The post-processing](#the-post-processing)
5. [Quickstart](#quickstart)
    - [Step by step installation with conda](#step-by-step-installation-with-conda)
    - [Instruction-output](#instruction-output)
    - [Instruction-input-output](#instruction-input-output)

# Overview
Self-instruct is an iterative method that helps LM improve their ability to follow natural language instructions. The idea is to use a seed set of manually-written instructio and use them to prompt the model to generate new instructions and their corresponding input-output instances. The method includes a filtering step to ensure the novelty of the generated task.

# Related work
Our implementation is inspired by the original [Self-instruct](https://github.com/yizhongw/self-instruct) method and recent updates including [Stanford's alpaca](https://github.com/tatsu-lab/stanford_alpaca/blob/) and [Code alpaca](https://github.com/sahil280114/codealpaca/). While the last two are identical, with the sole difference being the seed tasks used, the original work has a different mindset. As a matter of fact, self-instruct's author uses a set of seed tasks and prompt the model with some of them to make it generate instructions. Later on, the output to the generated instructions are found separately. Conversely, alpaca is all in one in the sense that the model generates instruction as well as input-output. It is prompted with the following template

```bash
### Instruction:
{instruction}

### Input:
{input}

### Output:
{output}
```
The advantage is that this all in one template allows to reduce the inference cost of the method, and the quality of the generated instances is not proven to be significantly impaired. We believe, intuitively, that if this prompting approach generates feasible instructions thanks to the obligation to have a sound input-output pair associated to it.

# Our approach

Our approach is focused on code use cases, therefore our modifications are mostly relevant for that framework.

## The prompting format
During our tests, we realized that, at least with "small" code models, the trigger words `Input:` and `Output:` tend to make them generate test cases instead. It is significantly impairing because given an instruction, we want a working implementation rather than a potentially buggy test case. In order to alleviate this issue, we decided to get rid of the "Input:" trigger word. We adopt an instruction-output format.

## The trigger words
Using "Instruction:", "Input:" and "Output:" seems to work well for `text-davinci-003` but how well does it work for other models? This parameter is definitely relevant for small models as this can have a huge impact. Following this intuition, we included in our code the possibility to change the trigger words that are used during the prompting. This allows to accomodate to every single model.

## The post-processing
How to select and post-process the instructions that are generated by prompting a model? In the original work, the instructions are generated iteratively, and we keep those with a rouge score stricly less than `0.7` with any previously generated instruction. This allows diversity in the dataset, at least in terms of how the instructions are worded. According to our experiments, it is still possible to generate a problem multiple times with a different formulation each time. We propose to extend take the curation further with multiple ideas.

## Further details
We modified the seed tasks to keep only those who are related to code. For that we combine the tasks from Code Alpaca (code tasks extrated from the original [seed tasks](https://github.com/yizhongw/self-instruct/blob/main/data/seed_tasks.jsonl) + some new tasks probably created by the repo's author) and some leetcode tasks. We have a total of `41` seed tasks.

# Quickstart

StarCoder was trained on GitHub code, thus it can be used to perform code generation. More precisely, the model can complete the implementation of a function or infer the following characters in a line of code. This can be done with the help of the ðŸ¤—'s [transformers](https://github.com/huggingface/transformers) library.

## Step by step installation with conda

Here, we present a step by step recipe that anybody can use in order to apply our self-instruct method on its prefered LLM in a conda environment.
Create a new conda environment and activate it
```bash
conda create -n env
conda activate env
```
Install the `pytorch` version compatible with your version of cuda [here](https://pytorch.org/get-started/previous-versions/), for example the following command works with cuda 11.6
```bash
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia
```
Install `transformers` and `accelerate`
```bash
conda install -c huggingface transformers 
pip install git+https://github.com/huggingface/accelerate.git
```
Do not forget to launch `accelerate config` in the terminal in order to configure you environment, for more the details see [accelerate](https://github.com/huggingface/accelerate).
We will also need `rouge-score` and `sentence-transformers`
```bash
pip install rouge-score
pip install sentence-transformers
```
Now we are ready to clone the repository and to start working 
```bash
git clone https://github.com/ArmelRandy/self-instruct
cd self-instruct
```
## Instruction - output

This part is related to the directory `instruction_io`. We prompt the model with the following template
```bash
### Instruction :
{instruction}

### Output :
{output}
```
For the instructions that provides an input (a code in case of a debugging task or a translation task), we concatenate the instruction and the input under the keyword "Instruction:", we then have 
```bash
### Instruction :
{instruction}{input}

### Output :
{output}
```
The possibility to change the trigger words `Instruction:` and `Output:` into other words such as `Request:` and `Answer:` respectively for example is given. However, the change has to be done directly in the code, as they trigger words are used as constant throughout the code.

```bash
accelerate launch instruction_output.py
    --batch_dir = "" \
    --num_instructions_to_generate 100 \
    --seed_tasks_path \
    --model_name_or_path \
    --max_length \
    --request_batch_size \
```

## Instruction - input - output

This part is related to the directory `instruction_iio`. It is the template as designed in Stanford's alpaca. The possibilty to change the trigger words is also provided, with the same limitations as those previously mentionned.

```bash
accelerate launch instruction_input_output.py
--batch_dir = "" \
--num_instructions_to_generate 100 \
--seed_tasks_path \
--model_name_or_path \
--max_length \
--request_batch_size \
```
